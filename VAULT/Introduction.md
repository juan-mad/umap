In today's digital world, huge volumes of data are being continuously generated in a wide range of fields: medicine, finance, IoT devices, education or social media, just to name a few (Ayesha, Reddy). With it comes an increasing interest in its analysis, a key aspect of domains such as statistics, data science, machine learning, and information visualisation (Espadoto). One of the problems that one might face when working with data is the number of attributes that are recorded with each sample or observation. A large number of attributes, also called features or dimensions, present several difficulties, not only due to higher computational complexity but also to the presence of noise and redundant information for the particular objective at hand, e.g. data visualisation or a classification task.

In order to overcome these issues, different techniques have been proposed in the literature. Among them, dimension reduction techniques aim to reduce the number of dimensions, or input variables,  by mapping or projecting the high dimensional data into a space of lower dimension. There are dozens of such methods, so one must consider what requirements should be met by the chosen technique: computational efficiency with respect to the number of observations or dimensions, the preservation of local or global structure of the data, robustness to variations in data and hyper-parameters... (Espadoto)

Principal component analysis, also known as PCA () is one of the most widespread dimension reduction techniques (Cunningham, Reddy). It works by finding orthogonal linear combinations (the principal components, or PCs) of the original variables with the largest variance: the first PC is the linear combination with the largest variance, the second PC is the linear combination with the second largest variance and orthogonal to the first PC, and so on (Fodor). There are as many PCs as original variables, but one usually selects a number of PCs that explain a desired percentage of the total variance.

While PCA works by finding a linear transformation of the original variables, there also exist non-linear algorithms for dimension reduction. Among them, t-SNE, which stands for t-distributed Stochastic Neighbour Embedding (van der Maaten), is arguably one of the best known and most widely used  techniques (Espadoto). It works by converting the high-dimensional data into a matrix of pairwise similarities, then used to visualise the resulting similarity data in lower dimension, typically in two or three dimensions. In this way, it is capable of capturing the local structure very well, while also revealing global structure such as the presence of clusters (van der Maaten).

One of t-SNE's contenders for its place as state-of-the-art algorithm is called Uniform Manifold Approximation and Projection (UMAP). Since its creation by {McInnes}(2018) it has gained popularity as a data visualisation technique in various fields, from (...) to (...). UMAP seeks to justify its workings on a solid mathematical basis, drinking from Riemannian geometry, algebraic topology and category theory. Some authors consider it superior to t-SNE in terms of visualisation () and computation time, as well as consistency with respect to its embeddings (). Others have argued that its performace relies heavily on the hyper-parameters and initialisation used (), and that recent optimisations of t-SNE, such as FIt-SNE (REFERENCE HERE!!!) are just as fast as UMAP (). Most interesting are the findings of {Böhm} and {Damrich}, who claim that there is a mismatch or disagreement between UMAP's theoretical description and its actual implementation.

In light of these different and opposed claims and assertions, this master thesis will deep dive into UMAP's inner workings. We study its performance under different hyper-parameter choices for different datasets and describe the choices taken in its original implementation (software UMAP). We also consider the claims of Böhm and Damrich by performing our own numerical experiments. Finally, we apply UMAP to particular datasets whose points are known to live in a manifold with a metric known a priori.

; at present, t-SNE (van der Maaten) is probably one of the best known and most utilised by practitioners (Espadoto). It is a non-linear technique that tries to capture the local and global structure of the original data.

Mention some otjher Non linear DR techniques, introduce UMAP


Main contributions: study of UMAP's HP, extension of Damrich work

Structure

